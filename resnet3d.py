# -*- coding: utf-8 -*-
"""ResNet3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tgUSrHVwRsEc4507NPx7RIFG2AHIshcl

Early Alzhimer's Detection
""""

!git clone https://github.com/blackpearl006/OASIS_nifti_Part_1
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_2
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_3
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_4
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_5
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_6
!git clone https://github.com/blackpearl006/OASIS_nifti_Part_7


import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import nibabel as nib
from sklearn.metrics import confusion_matrix
import seaborn as sns
from torch.utils.data import random_split
from torch.utils.data import DataLoader
from torchvision.transforms import ToTensor
from torch.nn.parallel import DataParallel
from torchvision.transforms import ToTensor
from torch.utils.data import WeightedRandomSampler
import torch.optim as optim


metadata = pd.read_csv('/content/kaggle.csv')
metadata.head(5)



class CustomDataset():
    def __init__(self, repo_paths, csv_path, num_samples = 200, transform = None):
        self.filelabels = self.load_class_labels(csv_path)
        self.file_paths = self.get_file_paths(repo_paths)
        self.transform = transform

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        file_path = self.file_paths[idx]
        nifti_data = nib.load(file_path)
        data = nifti_data.get_fdata()
        preprocessed_data = self.preprocess_data(data)

        preprocessed_tensor = torch.tensor(preprocessed_data, dtype=torch.float32)

        transformed_tensor = preprocessed_tensor.unsqueeze(0)

        file_id = file_path.split('/')[-1][0:13]
        if self.filelabels is not None:
            label = torch.tensor(self.filelabels[file_id], dtype=torch.float32).unsqueeze(0)
            return transformed_tensor, label
        else:
            return transformed_tensor

    def get_file_paths(self, paths):
        file_paths = []
        class_count = { 0: 0 , 1: 0}
        max_data_point = 100
        for path in paths:
            scans = os.listdir(path)
            scans = [scan for scan in scans if scan[:4] == 'OAS1']
#             print(f'Scans from path {path} have been found and the total count is : {len(scans)}')
            for folder_name in scans:
                folder_path = os.path.join(path, folder_name)  # Corrected variable name here
#                 print(f'Inside the folder {folder_path}')
                if os.path.isdir(folder_path) :
                    for file_name in os.listdir(folder_path):
                        file_path = os.path.join(folder_path, file_name)
#                         print(f'File Path: {file_path}')
                        label = self.filelabels[file_path.split('/')[-1][0:13]]
                        if class_count[label] < max_data_point :
                            class_count[label]+=1
                            file_paths.append(file_path)

            if sum(class_count.values()) == 2 * max_data_point: break
        return file_paths


    def load_class_labels(self, csv_path):
        df = pd.read_csv(csv_path)
        class_labels = {}
        for _, row in df.iterrows():
            id_value = row['ID']
            cdr_value = row['CDR']
            class_labels[id_value] = 0 if pd.isna(cdr_value) or float(cdr_value) == 0.0 else 1
        return class_labels

    def preprocess_data(self, data):
        mean = data.mean()
        std = data.std()
        normalized_data = (data - mean) / std
        return normalized_data

    def calculate_class_weights(self):
        class_count = {0: 0, 1: 0}
        total_len = len(self.file_paths)
        for file_path in self.file_paths:
            label = self.filelabels[file_path.split('/')[-1][0:13]]
            class_count[label]+=1
        inverse_class_weights = [1 / class_count[0],1 / class_count[1]]
        return(inverse_class_weights)

    def weighted_samples(self, num_samples):
        class_weights = self.calculate_class_weights()
        dataset_weights = []
        for file_path in self.file_paths:
            label = self.filelabels[file_path.split('/')[-1][0:13]]
            dataset_weights.append(class_weights[label])

        weighted_random_sampler = WeightedRandomSampler(dataset_weights, num_samples = num_samples, replacement = True)
        return weighted_random_sampler

data_dirs = [
    '/content/OASIS_nifti_Part_1',
    '/content/OASIS_nifti_Part_2',
    '/content/OASIS_nifti_Part_3',
    '/content/OASIS_nifti_Part_4',
    '/content/OASIS_nifti_Part_5',
    '/content/OASIS_nifti_Part_6',
]
csv_path = '/content/kaggle.csv'

custom_dataset = CustomDataset(data_dirs, csv_path)
custom_dataset

import random
rand_idx = [random.randint(1,100) for i in range(1)]
for idx in rand_idx:
    data, label = custom_dataset[idx]
    print("Data File Name:", custom_dataset.file_paths[idx].split('/')[-1][:13])
    print("Label:", label)
    print("Shape:", data.shape)

import matplotlib.pyplot as plt
import torch

fig, axs = plt.subplots(1, 2, figsize=(10, 5))

for num, (i, j) in enumerate(custom_dataset):
    slice_idx = i.shape[2] // 2  # Assuming the slice index is the last dimension


    slice_data = i[0, :, :, slice_idx].numpy()  # Convert to NumPy array

    label = int(j.item())
    ax = axs[num % 2]  # Alternate between two subplots
    ax.imshow(slice_data, cmap='gray')
    ax.set_title(f'MRI Slice : Label : {label}')

    if num % 2 == 1:
        plt.tight_layout()
        plt.show()
        if num >= 3:
            break

if num % 2 == 0:
    plt.tight_layout()
    plt.show()

val_size = int(len(custom_dataset) * 0.20)
train_size = len(custom_dataset) - val_size

train_ds, val_ds = random_split(custom_dataset, [train_size, val_size])

class_count = {0: 0, 1: 0}
total_count = 0
for data, label in train_ds:
    total_count+=1
    class_count[int(label.item())]+=1

inverse_class_weights = [1 / class_count[0],1 / class_count[1]]
inverse_class_weights

dataset_weights = []
for _, label in train_ds:
    dataset_weights.append(inverse_class_weights[int(label.item())])

num_samples = 400
weighted_random_sampler = WeightedRandomSampler(dataset_weights, num_samples = num_samples, replacement = True)

train_dl = DataLoader(train_ds, batch_size=4, sampler = weighted_random_sampler)
val_dl = DataLoader(val_ds, batch_size=4)
print('Using DataLoaders to Load the training and validation dataset')

print(f'Length of Training Dataset : {len(train_dl)*4}')
print(f'Length of Validation Dataset : {len(val_dl)*4}')
print(f'batch Size : {4}, With higher batch size we cannot train our model on GPU')


class BasicBlock3D(nn.Module):
    expansion = 1

    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(BasicBlock3D, self).__init__()
        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm3d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm3d(out_channels)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet18_3D(nn.Module):
    def __init__(self, num_classes=1):
        super(ResNet18_3D, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm3d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)

        # Residual blocks
        self.layer1 = self._make_layer(BasicBlock3D, 64, 2, stride=1)
        self.layer2 = self._make_layer(BasicBlock3D, 128, 2, stride=2)
        self.layer3 = self._make_layer(BasicBlock3D, 256, 2, stride=2)
        self.layer4 = self._make_layer(BasicBlock3D, 512, 2, stride=2)

        # Classifier
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.fc = nn.Linear(512 * BasicBlock3D.expansion, num_classes)

    def _make_layer(self, block, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or self.in_channels != out_channels * block.expansion:
            downsample = nn.Sequential(
                nn.Conv3d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm3d(out_channels * block.expansion),
            )

        layers = []
        layers.append(block(self.in_channels, out_channels, stride, downsample))
        self.in_channels = out_channels * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.in_channels, out_channels))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        return x


model = ResNet18_3D()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = model.to(device)

model = DataParallel(model)

class_count[1]

inverse_class_weights = torch.tensor([total_count / (2*class_count[1])],dtype=torch.float32)
inverse_class_weights

inverse_class_weights = inverse_class_weights.to(device)
criterion =  nn.BCEWithLogitsLoss(pos_weight = inverse_class_weights )

optimizer = optim.Adam(model.parameters(), lr=0.01)

num_epochs = 10

print(f'criterion =  nn.BCELoss(), optimizer = optim.Adam(model.parameters()), num_epochs = 25')

train_losses = []
val_losses = []
train_accu = []
val_accu = []
model_save_path = '/content/model.pth'

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_correct = 0
    train_total = 0

    for inputs, labels in train_dl:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        predicted = (outputs >= 0.5)
        train_total += labels.size(0)
        train_correct += (predicted == labels).sum().item()

        train_loss += loss.item()

    train_accuracy = 100 * train_correct / train_total


    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0

    with torch.no_grad():
        for inputs, labels in val_dl:
            inputs = inputs.to(device)
            labels = labels.to(device)


            outputs = model(inputs)
            loss = criterion(outputs, labels)


            predicted = (outputs >= 0.5)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

            val_loss += loss.item()

    val_accuracy = 100 * val_correct / val_total

    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accu.append(train_accuracy)
    val_accu.append(val_accuracy)

    print(f"Epoch [{epoch+1}/{num_epochs}]: "
          f"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, "
          f"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%")

torch.save(model, model_save_path)

torch.save(model.state_dict(), 'trained_model_state.pth')

from google.colab import files
files.download('/content/model.pth')

torch.cuda.empty_cache()

train_losses

epochs = list(range(1, len(train_losses) + 1))


plt.plot(epochs, train_losses, marker='o')
plt.title('Training Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim(0, 100)
plt.figure(facecolor='none')
plt.show()

plt.plot(epochs, val_losses, marker='o', color = 'orange')
plt.title('Validation Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim(0, 10)
plt.show()

plt.plot(epochs, train_accu, marker='o')
plt.plot(epochs, val_accu, marker='o')
plt.title('Traning and Validation Accuracy Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.ylim(0,100)
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

num_classes = 2
conf_matrix = np.zeros((num_classes, num_classes), dtype=int)

model.eval()

with torch.no_grad():
    for inputs, labels in train_dl:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        predicted = (outputs >= 0.5).int()  
        conf_matrix += confusion_matrix(labels.cpu(), predicted.cpu(), labels=[0, 1])


TP = conf_matrix[1, 1]
FP = conf_matrix[0, 1]
FN = conf_matrix[1, 0]

precision = TP / (TP + FP)
recall = TP / (TP + FN)


plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["True 0", "True 1"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.suptitle(f'Precision: {precision:.4f}, Recall: {recall:.4f}')
plt.show()

num_classes = 2
conf_matrix = np.zeros((num_classes, num_classes), dtype=int)

model.eval()

with torch.no_grad():
    for inputs, labels in val_dl:
        inputs = inputs.to(device)
        labels = labels.to(device)

        outputs = model(inputs)
        predicted = (outputs >= 0.5).int()  # Convert to int (0 or 1)

        conf_matrix += confusion_matrix(labels.cpu(), predicted.cpu(), labels=[0, 1])


TP = conf_matrix[1, 1]
FP = conf_matrix[0, 1]
FN = conf_matrix[1, 0]

precision = TP / (TP + FP)
recall = TP / (TP + FN)

plt.figure(figsize=(8, 6))
sns.set(font_scale=1.2)
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Predicted 0", "Predicted 1"],
            yticklabels=["True 0", "True 1"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.suptitle(f'Precision: {precision:.4f}, Recall: {recall:.4f}')
plt.show()

