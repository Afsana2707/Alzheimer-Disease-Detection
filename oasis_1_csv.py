# -*- coding: utf-8 -*-
"""OASIS 1 CSV

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cuxZtpWEgB2KPvfAB7AWYPHoZIMHAvyV
"""

from google.colab import drive
drive.mount('/content/MyDrive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
# sns.set()

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, roc_curve, auc, f1_score, precision_score

df = pd.read_csv('/content/MyDrive/MyDrive/Alzheimer disease/oasis_longitudinal.csv')
df.head(10)

df.shape

df.tail(10)

df['Visit'] = df['MRI ID'].str.extract(r'MR(\d+)').astype(int)
df.head()

df = df.loc[df['Visit']==1] # use first visit data only because of the analysis we're doing
df = df.reset_index(drop=True) # reset index after filtering first visit data
df['M/F'] = df['M/F'].replace(['F','M'], [0,1]) # M/F column
# df['Group'] = df['Group'].replace(['Converted'], ['Demented']) # Target variable
df['Group'] = df['Group'].replace(['Demented', 'Nondemented'], [1,0]) # Target variable

df = df.drop(['MRI ID', 'Visit', 'Hand'], axis=1) # Drop unnecessary columns

df = df.drop(['MR Delay'], axis=1)

df = df.drop(['Subject ID'], axis=1)

df.head(10)

df.shape

df.info()  # Get information about the dataset, such as data types and missing values

df.head()

# bar drawing function
def bar_chart(feature):
    VerymildDementia = df[df['Group']==1][feature].value_counts()
    Nondemented = df[df['Group']==0][feature].value_counts()
    MildDementia = df[df['Group'] == 2][feature].value_counts()
    ModerateDementia = df[df['Group'] ==3][feature].value_counts()
    df_bar = pd.DataFrame([VerymildDementia,Nondemented])
    df_bar.index = ['Very Mild Dementia','Non Demented']
    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))

# Gender  and  Group ( Femal=0, Male=1)
bar_chart('M/F')
plt.xlabel('Group')
plt.ylabel('Number of patients')
plt.legend()
plt.title('Gender and Demented rate')

# Get the counts of individuals without and with diabetes
countNotDemented = len(df[df.Group == 'Nondemented'])
countHaveverymild = len(df[df.Group == 'Demented'])
countHavemild = len(df[df.Group == 2])
countHavemoderate = len(df[df.Group == 3])
fig = plt.figure(figsize=(8,4))
# Create a bar plot with two bars
x=[countNotDemented,countHaveverymild,countHavemild,countHavemoderate]
plt.bar(['Non Demented', 'Dementia'], [countNotDemented, countHaveverymild],width=0.7, color=['cadetblue', 'crimson','green','blue'])
for i in range(len(x)):
    plt.annotate((x[i]), xy=(i, x[i]), ha='center', va='bottom')
# Add a title and axis labels
plt.title('Distribution of Dementia in dataset')
plt.xlabel('Group')
plt.ylabel('Count')
# Show the plot
plt.show()

"""# 5. Data Preprocessing
---
We identified 8 rows with missing values in SES column. We deal with this issue with 2 approaches. One is just to drop the rows with missing values. The other is to replace the missing values with the corresponing values, also known as 'Imputation'. Since we have only 150 data, I assume imputation would help the performance of our model.
"""

# Check missing values by each column
pd.isnull(df).sum()

"""## 5.B Imputation

Scikit-learn provides package for imputation [6], but we do it manually. Since the *SES* is a discrete variable, we use median for the imputation.
"""

from sklearn.impute import KNNImputer
names = ['Group', 'M/F', 'Age', 'Educ', 'SES', 'MMSE', 'CDR', 'eTIV','nWBV','ASF']
dataset = df
dataset[['SES']] = dataset[['SES']].replace(0, np.nan)
dataset[['SES']] = dataset[['SES']].replace(np.nan, 0)
# imputer = KNNImputer(n_neighbors=5)
# dataset_imputed = imputer.fit_transform(dataset)
# dataset_imputed = pd.DataFrame(dataset_imputed, columns=names)
# dataset_imputed = dataset_imputed.round(2)
# df1 = dataset_imputed

# df = df.dropna()

df=dataset

df["SES"].fillna(df.groupby("Educ")["SES"].transform("median"), inplace=True)

pd.isnull(df['SES']).value_counts()

print(df.describe())

correlation_matrix = df.corr()
plt.figure(figsize=(8, 5))
sns.heatmap(correlation_matrix, annot=True, cmap="YlGnBu")
plt.title("Correlation Matrix Heatmap")
plt.show()

df.corr()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Age', kde=True)
plt.title("Histogram of Age")
plt.show()

# Example: Histogram of EDUC
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='Educ', kde=True)
plt.title("Histogram of EDUC")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='SES', kde=True)
plt.title("Histogram of SES")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='MMSE', kde=True)
plt.title("Histogram of MMSE")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='CDR', kde=True)
plt.title("Histogram of CDR")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='eTIV', kde=True)
plt.title("Histogram of eTIV")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='nWBV', kde=True)
plt.title("Histogram of nWBV")
plt.show()

# Example: Histogram of Age
plt.figure(figsize=(8, 6))
sns.histplot(data=df, x='ASF', kde=True)
plt.title("Histogram of ASF")
plt.show()

sns.pairplot(data=df, hue = 'Group')
plt.title("Pair Plot")
plt.show()

# Joint plot example (e.g., Age vs. MMSE)
sns.jointplot(data=df, x='CDR', y='Educ', kind='reg')
plt.title("Joint Plot CDR vs. EDUC")
plt.show()

sns.jointplot(data=df, x='SES', y='CDR', kind='reg')
plt.title("Joint Plot: SES vs CDR")
plt.show()

# Draw scatter plot between EDUC and SES
x = df['Educ']
y = df['SES']

ses_not_null_index = y[~y.isnull()].index
x = x[ses_not_null_index]
y = y[ses_not_null_index]

# Draw trend line in red
z = np.polyfit(x, y, 1)
p = np.poly1d(z)
plt.plot(x, y, 'go', x, p(x), "r--")
plt.xlabel('Education Level(EDUC)')
plt.ylabel('Social Economic Status(SES)')

plt.show()

df.groupby(['Educ'])['SES'].median()

"""## 5.C Splitting Train/Validation/Test Sets"""

X = df.drop(['Group'],axis=1)
y = df['Group']

X = X.drop(['CDR'],axis=1)

# from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
# # Method 2: Standardization (Z-score normalization)
# numerical_features = ['eTIV', 'nWBV', 'ASF']
# Z = X[numerical_features]
# scaler_standard = StandardScaler()
# X_standard = scaler_standard.fit_transform(Z)
# normalized_standard = pd.DataFrame(X_standard, columns=numerical_features)
# normalized_data = pd.concat([df.drop(numerical_features, axis=1), normalized_standard], axis=1)

# X = normalized_data
X.head(10)

X.shape

y.shape

X.head(10)
X.tail(10)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# 6. MODEL
---
"""

y_train = y_train.replace('Converted',0)

y_test = y_test.replace('Converted',0)

"""## 6.B Logistic Regression
The parameter C, inverse of regularization strength.

Tuning range: [0.001, 0.1, 1, 10, 100]
"""

acc = [] # list to store all performance metric

lr = LogisticRegression(C=0.1, max_iter = 500)
lr.fit(X_train,y_train)
y_pred = lr.predict(X_test)
accuracy_lr = round(accuracy_score(y_pred, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred, y_test), 3))
print("Precison : ",round(precision_score(y_pred, y_test), 3))
print("F1-score : ",round(f1_score(y_pred, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred, y_test), 3))

svm = SVC(kernel='linear',C=10, probability=True)
svm.fit(X_train,y_train)
y_pred1 = svm.predict(X_test)
accuracy_svm = round(accuracy_score(y_pred1, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred1, y_test), 3))
print("Precison : ",round(precision_score(y_pred1, y_test), 3))
print("F1-score : ",round(f1_score(y_pred1, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred1, y_test), 3))

dt = DecisionTreeClassifier(max_depth=3, ccp_alpha=0.1)
dt.fit(X_train,y_train)
y_pred2 = dt.predict(X_test)
accuracy_dt = round(accuracy_score(y_pred2, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred2, y_test), 3))
print("Precison : ",round(precision_score(y_pred2, y_test), 3))
print("F1-score : ",round(f1_score(y_pred2, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred2, y_test), 3))

rf = RandomForestClassifier(n_estimators=60, max_depth=6, min_samples_split=4, ccp_alpha=0.01)
rf.fit(X_train, y_train)
y_pred3 = rf.predict(X_test)
accuracy_rf = round(accuracy_score(y_pred3, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred3, y_test), 3))
print("Precison : ",round(precision_score(y_pred3, y_test), 3))
print("F1-score : ",round(f1_score(y_pred3, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred3, y_test), 3))

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes = (8, 20))
mlp.fit(X_train, y_train)
y_pred4 = mlp.predict(X_test)
print("Accuracy : ",round(accuracy_score(y_pred4, y_test), 3))
print("Precison : ",round(precision_score(y_pred4, y_test), 3))
print("F1-score : ",round(f1_score(y_pred4, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred4, y_test), 3))

from sklearn.ensemble import AdaBoostClassifier
adaboost = AdaBoostClassifier(estimator=dt, n_estimators=15, random_state=42)#, learning_rate=2)
adaboost.fit(X_train, y_train)
y_pred5 = adaboost.predict(X_test)
accuracy_ada = round(accuracy_score(y_pred5, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred5, y_test), 3))
print("Precison : ",round(precision_score(y_pred5, y_test), 3))
print("F1-score : ",round(f1_score(y_pred5, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred5, y_test), 3))

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)
y_pred6 = knn.predict(X_test)
accuracy_knn = round(accuracy_score(y_pred6, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred6, y_test), 3))
print("Precison : ",round(precision_score(y_pred6, y_test), 3))
print("F1-score : ",round(f1_score(y_pred6, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred6, y_test), 3))

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred7 = gnb.predict(X_test)
accuracy_gnb = round(accuracy_score(y_pred7, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred7, y_test), 3))
print("Precison : ",round(precision_score(y_pred7, y_test), 3))
print("F1-score : ",round(f1_score(y_pred7, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred7, y_test), 3))

import xgboost as xgb
xgboost = xgb.XGBClassifier()
xgboost.fit(X_train, y_train)
y_pred8 = xgboost.predict(X_test)
accuracy_xgb = round(accuracy_score(y_pred8, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred8, y_test), 3))
print("Precison : ",round(precision_score(y_pred8, y_test), 3))
print("F1-score : ",round(f1_score(y_pred8, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred8, y_test), 3))

import lightgbm as lgb
lgbm = lgb.LGBMClassifier()
lgbm.fit(X_train, y_train)
y_pred9 = lgbm.predict(X_test)
accuracy_lgb = round(accuracy_score(y_pred9, y_test), 3)
print("Accuracy : ",round(accuracy_score(y_pred9, y_test), 3))
print("Precison : ",round(precision_score(y_pred9, y_test), 3))
print("F1-score : ",round(f1_score(y_pred9, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred9, y_test), 3))

!pip install catboost

from catboost import CatBoostClassifier
cat = CatBoostClassifier(learning_rate=0.5, n_estimators=100)
cat.fit(X_train, y_train)
y_pred10 = cat.predict(X_test)
print("Accuracy : ",round(accuracy_score(y_pred10, y_test), 3))
print("Precison : ",round(precision_score(y_pred10, y_test), 3))
print("F1-score : ",round(f1_score(y_pred10, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred10, y_test), 3))

from sklearn.linear_model import PassiveAggressiveClassifier
pac = PassiveAggressiveClassifier(C=0.1, max_iter=200, n_iter_no_change = 3)
pac.fit(X_train, y_train)
y_pred11 = pac.predict(X_test)
print("Accuracy : ",round(accuracy_score(y_pred11, y_test), 3))
print("Precison : ",round(precision_score(y_pred11, y_test), 3))
print("F1-score : ",round(f1_score(y_pred11, y_test), 3))
print("Recall-score : ",round(recall_score(y_pred11, y_test), 3))

X.shape

data = { 'SVM': y_pred1, 'AdaBoost': y_pred5, 'GNaiveBayes': y_pred7}
predicted_values = pd.DataFrame(data)

predicted_values['FinalPrediction'] = predicted_values.median(axis=1)#[0]
predicted_values['FinalPrediction'] = predicted_values['FinalPrediction'].astype(int)
y_predicted = predicted_values['FinalPrediction']

accuracy_vote = round(accuracy_score(y_predicted, y_test), 3)
precision = round(precision_score(y_predicted, y_test), 3)
f1 = round(f1_score(y_predicted, y_test), 3)
recall = round(recall_score(y_predicted, y_test), 3)

print("Accuracy: ", accuracy_vote)
print("Precision: ", precision)
print("F1-score: ", f1)
print("Recall-score: ", recall)

model_names = ['LR', 'SVM', 'DT', 'AdaBoost', 'RF', 'KNN', 'XGB', 'GNB', 'LGB','Vote']
accuracies = [accuracy_lr, accuracy_svm, accuracy_dt, accuracy_ada, accuracy_rf, accuracy_knn, accuracy_xgb, accuracy_gnb, accuracy_lgb, accuracy_vote]

fig = plt.figure(figsize=(10,5))
plt.bar(model_names, accuracies, color='cadetblue', width=0.6)

for i in range(len(model_names)):
    plt.annotate(str(round(accuracies[i], 2)), xy=(i, accuracies[i]), ha='center', va='bottom')

plt.title('Comparison of Model Accuracies')
plt.xlabel('Model')
plt.ylabel('Accuracy')

plt.xticks(rotation=80) # rotate x-axis labels by 45 degrees

plt.show()

from sklearn.metrics import roc_curve, auc
from matplotlib import pyplot as plt

# define your models and data
# models = [lr_model,svm_model,dt_model,adaboost_lr,adaboost_svm,adaboost_dt,ada_bias_lr,ada_bias_svm,ada_bias_dt,svm_rbf,model,knn,rf,nb]
models = [lr, svm, dt, adaboost, rf, knn,  gnb]
data = [X_test, X_test, X_test, X_test, X_test, X_test, X_test]
labels = ['LR','SVM','DT','AdaBoost', 'RF', 'KNN',  'GNB']
# iterate through the models and plot the curves
colors = ['green','blue','pink','coral','darkgreen','yellow','crimson']
fig = plt.figure(figsize=(8,3))

for i in range(len(models)):
    model = models[i]
    X_test = data[i]
    y_score = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_score)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr,color=colors[i], label=f'{labels[i]}\n(AUC={roc_auc:.3f})')

plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

# add axis labels and legend
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.6), ncol=4)


# display the plot
plt.show()