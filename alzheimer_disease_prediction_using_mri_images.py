# -*- coding: utf-8 -*-
"""Alzheimer Disease prediction using MRI images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pCTVVWcWRZWzPQezr_hZV9L20ps7IM4Q
"""

import warnings
warnings.filterwarnings('ignore')
from google.colab import drive
drive.mount('/content/MyDrive')
import os
from os import listdir
import pathlib
from random import randint
import numpy as np
from numpy import asarray
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.image import imread
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
!pip install tensorflow
!pip install tensorflow-addons
import tensorflow as tf
import keras
import tensorflow_addons as tfa
from keras.utils import load_img,img_to_array
from keras.models import Sequential
from tensorflow.keras.applications.inception_v3 import InceptionV3
from keras.layers import MaxPooling2D,Dropout,Dense,Input,Conv2D,Flatten,Conv2DTranspose
from keras.layers import GlobalAveragePooling2D,MaxPool2D,BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint, EarlyStopping
from keras.utils import plot_model

folder = '/content/MyDrive/MyDrive/Alzheimer disease/Dataset'
folder_path = pathlib.Path(folder)

photo = load_img('/content/MyDrive/MyDrive/Alzheimer disease/Dataset/Moderate_Demented/moderate_2.jpg')
print(photo)
photo

for file in listdir(folder):
    print(file)

plt.figure(figsize=(7,7),)
j =0
files = ['Moderate_Dementia','Mild_Dementia','Very_mild_Dementia','Non_Demented']
for file in listdir(folder):
    i =0
    if file in files:
      for image in listdir(folder + '/'+file):
        if i ==1:
            break;
        img = imread(folder +'/' + file + '/' + image)
        ax = plt.subplot(2,2,j+1)
        plt.imshow(img)
        plt.title(image)
        plt.axis('off')
        j = j+1
        i = i +1
        plt.show()

# counting the number of images in each category
for file in listdir(folder):
    i =0
    for image in listdir(folder + '/'+file):
        i = i +1
    print(file , i)

"""#### Here we can see that the dataset is completely imbalanced with different number of images in each category"""

# Creating the image datagenerator to have more samples
IMG_SIZE = 128
DIM = (IMG_SIZE, IMG_SIZE)

ZOOM = [.99, 1.01]
BRIGHT_RANGE = [0.8, 1.2]
HORZ_FLIP = True
FILL_MODE = "constant"
DATA_FORMAT = "channels_last"

train_generator = ImageDataGenerator(rescale = 1./255, brightness_range=BRIGHT_RANGE, zoom_range=ZOOM,
                                     data_format=DATA_FORMAT, fill_mode=FILL_MODE, horizontal_flip=HORZ_FLIP)
train_data_gen = train_generator.flow_from_directory(directory=folder, target_size=DIM, batch_size=6500, shuffle=False)

CLASSES = list(train_data_gen.class_indices.keys())

def show_images(generator,y_pred=None):
    # get image lables
    labels =dict(zip([0,1,2,3], CLASSES))

    # get a batch of images
    x,y = generator.next()

    # display a grid of 9 images
    plt.figure(figsize=(7, 7))
    if y_pred is None:
        for i in range(9):
            ax = plt.subplot(3, 3, i + 1)
            idx = randint(0, 6400)
            plt.imshow(x[idx])
            plt.axis("off")
            plt.title("Class:{}".format(labels[np.argmax(y[idx])]))
    else:
        for i in range(9):
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(x[i])
            plt.axis("off")
            plt.title("Actual:{} \nPredicted:{}".format(labels[np.argmax(y[i])],labels[y_pred[i]]))

# Display Train Images
show_images(train_data_gen)

train_data, train_labels = train_data_gen.next()

train_data.shape, train_labels.shape

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Assuming you have your data and labels loaded and preprocessed as train_data and train_labels

# Combine the data and labels into a single dataset
combined_data = train_data.reshape(-1, IMG_SIZE * IMG_SIZE * 3)
combined_labels = train_labels

# # Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    combined_data, combined_labels, test_size=0.3, random_state=42
)

# Instantiate the SMOTE object
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Fit and apply SMOTE to the training data only
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Now you have a balanced training dataset (X_train_resampled, y_train_resampled)
# and an unchanged testing dataset (X_test, y_test)

"""Since the data in each class is imbalanced we need to oversample the data"""

from imblearn.over_sampling import SMOTE

# Instantiate the SMOTE object
sm = SMOTE(random_state=42)

# Reshape your data if needed
train_data = train_data.reshape(-1, IMG_SIZE * IMG_SIZE * 3)

# Apply SMOTE to oversample the minority classes
train_data, train_labels = sm.fit_resample(train_data, train_labels)

# Check the shape of the resampled data
print(train_data.shape, train_labels.shape)

train_data = train_data.reshape(-1, IMG_SIZE, IMG_SIZE, 3)
print(train_data.shape, train_labels.shape)

train_data, test_data, train_labels,test_labels = train_test_split(train_data, train_labels, test_size = 0.2, random_state=42)

train_data, val_data, train_labels,val_labels = train_test_split(train_data, train_labels, test_size = 0.2, random_state=42)

# Defining convolutional blocks
def conv_block(filters, act='relu'):

    block = Sequential()
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(Conv2D(filters, 3, activation=act, padding='same'))
    block.add(BatchNormalization())
    block.add(MaxPool2D())

    return block

# defining dense blocks
def dense_block(units, dropout_rate, act='relu'):
    block = Sequential()
    block.add(Dense(units, activation=act))
    block.add(BatchNormalization())
    block.add(Dropout(dropout_rate))

    return block

IMAGE_SIZE = [128,128]
act = 'relu'

model = Sequential([
        Input(shape=(*IMAGE_SIZE, 3)),
        Conv2D(16, 3, activation=act),
        Conv2D(16, 3, activation=act),
        MaxPool2D(),
        conv_block(32),
        conv_block(64),
        conv_block(128),
        Dropout(0.2),
        conv_block(256),
        Dropout(0.2),
        Flatten(),
        dense_block(512, 0.7),
        dense_block(128, 0.5),
        dense_block(64, 0.3),
        Dense(4, activation='softmax')
    ], name = "cnn_model")

METRICS = [tf.keras.metrics.CategoricalAccuracy(name='acc'),
           tf.keras.metrics.AUC(name='auc'),
           tfa.metrics.F1Score(num_classes=4)]

model.compile(optimizer='adam',
              loss=tf.losses.CategoricalCrossentropy(),
              metrics=METRICS)

model.summary()

plot_model(model)

print(train_data.shape)
print(train_labels.shape)

print(val_data.shape)
print(val_labels.shape)

from keras.utils import to_categorical

# Assuming train_labels and val_labels are categorical labels (e.g., 0, 1, 2, 3)
train_labels = to_categorical(train_labels, num_classes=4)
val_labels = to_categorical(val_labels, num_classes=4)
test_labels = to_categorical(test_labels, num_classes=4)

from tensorflow.keras.callbacks import ModelCheckpoint
checkpoint_path = '/content/MyDrive/MyDrive/Alzheimer disease/Dataset/model_checkpoint.h5'
model_checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True)
CALLBACKS = [
    EarlyStopping(monitor='acc', min_delta=0.01, patience=5, mode='max'),
    model_checkpoint
]

EPOCHS = 10
tf.config.run_functions_eagerly(True)
#
history = model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=EPOCHS,callbacks=CALLBACKS)

test_scores = model.evaluate(test_data, test_labels)
print("Testing Accuracy: %.2f%%"%(test_scores[1] * 100))

#Plotting the trend of the metrics during training

fig, ax = plt.subplots(1, 3, figsize = (30, 5))
ax = ax.ravel()

for i, metric in enumerate(["acc", "auc", "loss"]):
    ax[i].plot(history.history[metric])
    ax[i].plot(history.history["val_" + metric])
    ax[i].set_title("Model {}".format(metric))
    ax[i].set_xlabel("Epochs")
    ax[i].set_ylabel(metric)
    ax[i].legend(["train", "val"])



"""## Transfer learning with Inceptionv3 model"""

inception = InceptionV3(input_shape=(128,128,3),include_top=False,weights ='imagenet')

for layer in inception.layers:
    layer.trainable = False

inception_model = Sequential([
        inception,
        Dropout(0.5),
        GlobalAveragePooling2D(),
        Flatten(),
        BatchNormalization(),
        Dense(512, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dropout(0.5),
        BatchNormalization(),
        Dense(4, activation='softmax')
    ], name = "inception_cnn_model")

inception_model.compile(optimizer='rmsprop',
                              loss=tf.losses.CategoricalCrossentropy(),
                              metrics=METRICS)

inception_model.summary()

plot_model(inception_model)

INCEPT_CALLBACKS = [
    ModelCheckpoint(filepath='incpt_model_checkpoint.h5', save_best_only=True),
    EarlyStopping(patience=3)
]

incept_history = inception_model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10,callbacks= INCEPT_CALLBACKS)

incept_test_scores = inception_model.evaluate(test_data, test_labels)
print("Testing Accuracy: %.2f%%"%(incept_test_scores[1] * 100))

#Plotting the trend of the metrics during training

fig, ax = plt.subplots(1, 3, figsize = (30, 5))
ax = ax.ravel()

for i, metric in enumerate(["acc", "auc", "loss"]):
    ax[i].plot(incept_history.history[metric])
    ax[i].plot(incept_history.history["val_" + metric])
    ax[i].set_title("Model {}".format(metric))
    ax[i].set_xlabel("Epochs")
    ax[i].set_ylabel(metric)
    ax[i].legend(["train", "val"])

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetB0  # You can choose a different variant (B0, B1, B2, ..., B7)
from sklearn.metrics import accuracy_score
# Define the number of models in the ensemble
num_models = 1

# Create a list to hold the individual models
models = [model]

# Create and compile each model
for i in range(num_models):
    # Define the input layer
    input_layer = Input(shape=(80, 80, 3))  # Adjust input shape as needed

    # Load the EfficientNet architecture
    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=input_layer)

    # Freeze the base model layers
    for layer in base_model.layers:
        layer.trainable = False

    # Add your custom classification head
    x = GlobalAveragePooling2D()(base_model.output)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation='relu')(x)
    x = Dropout(0.5)(x)
    output = Dense(4, activation='softmax')(x)  # Adjust num_classes

    # Create the model
    model = Model(inputs=input_layer, outputs=output)

    # Compile the model
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    models.append(model)

# Train each model individually
for i, model in enumerate(models):
    print(f"Training Model {i+1}...")
    # Train the model with your dataset
    # Replace X_train and y_train with your training data and labels
    model.fit(train_data, train_labels, epochs=2, batch_size=32, validation_data=(val_data,val_labels))  # Adjust hyperparameters as needed

# Ensemble predictions
def ensemble_predictions(models, X):
    y_pred = [model.predict(X) for model in models]
    y_pred_ensemble = tf.reduce_mean(y_pred, axis=0)
    return y_pred_ensemble

# Make predictions using the ensemble
X_test = test_data  # Load your test data
y_pred_ensemble = ensemble_predictions(models, X_test)

# Evaluate the ensemble's performance
ensemble_accuracy = model.evaluate(test_data, test_labels)
print(f"Ensemble Accuracy: {ensemble_accuracy}")
individual_predictions = [model.predict(X_test) for model in models]

print(individual_predictions)